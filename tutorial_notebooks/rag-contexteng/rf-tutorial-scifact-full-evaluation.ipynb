{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06d0c6d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/docs/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> ‚≠ê\n",
    "<br/>\n",
    "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fc36b",
   "metadata": {},
   "source": [
    "### RapidFire AI RAG/Context Engineering Tutorial Use Case: SciFact Q&A Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fce7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.evals.automl import (\n",
    "    List,\n",
    "    RFLangChainRagSpec,\n",
    "    RFOpenAIAPIModelConfig,\n",
    "    RFPromptManager,\n",
    "    RFGridSearch,\n",
    ")\n",
    "import re, json\n",
    "from typing import List as listtype, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c69cf-96ea-47e6-87c3-e3945758387d",
   "metadata": {},
   "source": [
    "##### ‚ö†Ô∏è API Cost Considerations\n",
    "This notebook runs 4 configurations concurrently on a downsampled dataset of 256 examples.\n",
    "Estimated Costs:\n",
    "- Current run (downsampled): \\$5 \n",
    "- Full set: \\$45\n",
    "\n",
    "> üí° **Tip:** Monitor your API usage to avoid unexpected charges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16327b",
   "metadata": {},
   "source": [
    "### Load Dataset and Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee571098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dataset directory is now in tutorial_notebooks/evals/datasets\n",
    "data = []\n",
    "with open(\"datasets/scifact/queries.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "for d in data:\n",
    "    if d[\"metadata\"]:\n",
    "        for info in d[\"metadata\"].values():\n",
    "            tags = set([meta[\"label\"] for meta in info])\n",
    "            assert len(tags) == 1\n",
    "            d[\"label\"] = tags.pop()  # SUPPORT or CONTRADICT\n",
    "    else:\n",
    "        d[\"label\"] = \"NOINFO\"\n",
    "\n",
    "scifact_dataset = {\n",
    "    \"query\": [d[\"text\"] for d in data],\n",
    "    \"query_id\": [d[\"_id\"] for d in data],\n",
    "    \"label\": [d[\"label\"] for d in data],\n",
    "}\n",
    "scifact_dataset = Dataset.from_dict(scifact_dataset).shuffle(seed=42).select(range(256))\n",
    "\n",
    "qrels = pd.read_csv(\"datasets/scifact/qrels.tsv\", sep=\"\\t\")\n",
    "qrels = qrels.rename(\n",
    "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
    ")\n",
    "qrels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28399289",
   "metadata": {},
   "source": [
    "### Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70816920",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(experiment_name=\"exp1-scifact-full-evaluation\", mode=\"evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a21ee",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for LangChain part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Dict\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def metadata_func(record: Dict, metadata: Dict):\n",
    "    metadata[\"corpus_id\"] = int(record.get(\"_id\"))\n",
    "    metadata[\"title\"] = record.get(\"title\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def custom_template(doc: Document) -> str:\n",
    "    return f\"{doc.metadata['title']}: {doc.page_content}\"\n",
    "\n",
    "\n",
    "# CPU-based RAG\n",
    "rag_cpu = RFLangChainRagSpec(\n",
    "    document_loader=DirectoryLoader(\n",
    "        path=\"datasets/scifact/\",\n",
    "        glob=\"corpus.jsonl\",\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"metadata_func\": metadata_func,  # store the document id\n",
    "            \"json_lines\": True,\n",
    "            \"text_content\": False,\n",
    "        },\n",
    "        sample_seed=1337,\n",
    "    ),\n",
    "    text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        encoding_name=\"gpt2\", chunk_size=512, chunk_overlap=32\n",
    "    ),\n",
    "    embedding_cls=OpenAIEmbeddings,\n",
    "    embedding_kwargs={\"model\": \"text-embedding-3-small\", \"api_key\": OPENAI_API_KEY},\n",
    "    vector_store=None,  # uses FAISS by default\n",
    "    search_type=List([\"similarity\", \"mmr\"]),  # 2 different search types\n",
    "    search_kwargs={\"k\": 10},\n",
    "    reranker_cls=CrossEncoderReranker,\n",
    "    reranker_kwargs={\n",
    "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cpu\"},\n",
    "        \"top_n\": 5,\n",
    "    },\n",
    "    enable_gpu_search=False,\n",
    "    document_template=custom_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fb0a8",
   "metadata": {},
   "source": [
    "### Define Data Processing and Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"\"\"\n",
    "You are a helpful assistant that can verify scientific claims. You will be given a scientific claim and a list of documents that are potentially relevant to the claim. Your job is to determine whether the claim is supported, contradicted, or not addressed by the evidence. You will do so by responding with one of the following options:\n",
    "\n",
    "- SUPPORT: If the evidence supports the claim.\n",
    "- CONTRADICT: If the evidence contradicts the claim.\n",
    "- NOINFO: If the evidence does not provide enough information to determine whether the claim is supported or contradicted.\n",
    "\n",
    "You will output your final answer after reasoning through the evidence. The final answer should be one of the three options and should be formatted as follows:\n",
    "\n",
    "Reasoning for the answer #### ANSWER\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "claim: High cardiopulmonary fitness causes increased mortality rate.\n",
    "\n",
    "evidence:\n",
    "One consequence of inactivity, low cardiorespiratory fitness, is an established risk factor for cardiovascular disease (CVD) morbidity and mortality, but the prevalence of cardiorespiratory fitness has not been quantified in representative US population samples.\n",
    "\n",
    "Cardiosphere-derived cells transplanted into chick embryos migrated to the truncus arteriosus and cardiac outflow tract and contributed to dorsal root ganglia, spinal nerves, and aortic smooth muscle cells. Lineage studies using double transgenic mice encoding protein 0\\u2013Cre/Floxed-EGFP revealed undifferentiated and differentiated neural crest-derived cells in the fetal myocardium\n",
    "\n",
    "Patients undergoing dialysis have a substantially increased risk of cardiovascular mortality and morbidity. Although several trials have shown the cardiovascular benefits of lowering blood pressure in the general population, there is uncertainty about the efficacy and tolerability of reducing blood pressure in patients on dialysis\n",
    "\n",
    "Response: The evidence suggests that low cardiorespiratory fitness is a known risk factor for cardiovascular disease and therefore the claim is contradicted. #### CONTRADICT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc17276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_preprocess_fn(\n",
    "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
    ") -> Dict[str, listtype]:\n",
    "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
    "\n",
    "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
    "    retrieved_documents = [\n",
    "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
    "    ]\n",
    "    serialized_context = rag.serialize_documents(all_context)\n",
    "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
    "\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\\nClaim:\\n{question}. \\nEvidence:\\n{context}. \\nYour response:\",\n",
    "                },\n",
    "            ]\n",
    "            for question, context in zip(batch[\"query\"], serialized_context)\n",
    "        ],\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "        **batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_solution(answer):\n",
    "    solution = re.search(r\"####\\s*(SUPPORT|CONTRADICT|NOINFO)\", answer, re.IGNORECASE)\n",
    "    if solution is None:\n",
    "        return \"INVALID\"\n",
    "    return solution.group(1).upper()\n",
    "\n",
    "\n",
    "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
    "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
    "    # Get ground truth documents for each query; can be done in preprocess_fn too but done here for clarity\n",
    "    batch[\"ground_truth_documents\"] = [\n",
    "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
    "        for query_id in batch[\"query_id\"]\n",
    "    ]\n",
    "    batch[\"answer\"] = [extract_solution(answer) for answer in batch[\"generated_text\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb16c3",
   "metadata": {},
   "source": [
    "### Define Custom Eval Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=3):\n",
    "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
    "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
    "    ideal_length = min(k, len(expected_docs))\n",
    "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
    "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
    "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
    "    rr = 0\n",
    "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
    "        if retrieved_doc in expected_docs:\n",
    "            rr = 1 / (i + 1)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "def compute_accuracy(predictions, ground_truth):\n",
    "    \"\"\"Label prediction accuracy: SUPPORT, CONTRADICT, NOINFO\"\"\"\n",
    "    return sum(1 for pred, gt in zip(predictions, ground_truth) if pred == gt) / len(predictions)\n",
    "\n",
    "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
    "\n",
    "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs, acc = 0, [], [], [], [], [], []\n",
    "    total_queries = len(batch[\"query\"])\n",
    "\n",
    "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
    "        expected_set = set(gt)\n",
    "        retrieved_set = set(pred[:3])\n",
    "\n",
    "        true_positives = len(expected_set.intersection(retrieved_set))\n",
    "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
    "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=3))\n",
    "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
    "    \n",
    "    accuracy = compute_accuracy(batch[\"answer\"], batch[\"label\"])\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
    "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
    "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
    "        \"NDCG@3\": {\"value\": sum(ndcgs) / total_queries},\n",
    "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
    "        \"Accuracy\": {\"value\": accuracy}\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_accumulate_metrics_fn(\n",
    "    aggregated_metrics: Dict[str, listtype],\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
    "\n",
    "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
    "    total_queries = sum(num_queries_per_batch)\n",
    "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@3\", \"MRR\", \"Accuracy\"]\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        **{\n",
    "            metric: {\n",
    "                \"value\": sum(\n",
    "                    m[\"value\"] * queries\n",
    "                    for m, queries in zip(\n",
    "                        aggregated_metrics[metric], num_queries_per_batch\n",
    "                    )\n",
    "                )\n",
    "                / total_queries,\n",
    "                \"is_algebraic\": True,\n",
    "                \"value_range\": (0, 1),\n",
    "            }\n",
    "            for metric in algebraic_metrics\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57887bc",
   "metadata": {},
   "source": [
    "### Define Partial Multi-Config Knobs for OpenAI Generator part of RAG Pipeline using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 openai configs with different sizes of generator models and different reasoning levels\n",
    "openai_config1 = RFOpenAIAPIModelConfig(\n",
    "    client_config={\"api_key\": OPENAI_API_KEY, \"max_retries\": 2},\n",
    "    model_config={\n",
    "        \"model\": \"gpt-5-mini\",\n",
    "        \"max_completion_tokens\": 4096,\n",
    "        \"reasoning_effort\": \"high\",\n",
    "    },\n",
    "    rpm_limit=10_000, # Request per minute (RPM) needs to be set based on your account tier and the specific model used\n",
    "    tpm_limit=10_000_000, # Token per minute (TPM) needs to be set based on your account tier and the specific model used\n",
    "    rag=rag_cpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "openai_config2 = RFOpenAIAPIModelConfig(\n",
    "    client_config={\"api_key\": OPENAI_API_KEY, \"max_retries\": 2},\n",
    "    model_config={\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"max_completion_tokens\": 1024,\n",
    "    },\n",
    "    rpm_limit=10_000, # Request per minute (RPM) needs to be set based on your account tier and the specific model used\n",
    "    tpm_limit=2_000_000, # Token per minute (TPM) needs to be set based on your account tier and the specific model used\n",
    "    rag=rag_cpu,\n",
    "    prompt_manager=None,\n",
    ")\n",
    "\n",
    "\n",
    "config_set = {\n",
    "    \"openai_config\": List(\n",
    "        [openai_config1, openai_config2]\n",
    "    ),  # Each represents 2 configs\n",
    "    \"batch_size\": batch_size,\n",
    "    \"preprocess_fn\": sample_preprocess_fn,\n",
    "    \"postprocess_fn\": sample_postprocess_fn,\n",
    "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
    "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
    "    \"online_strategy_kwargs\": {\n",
    "        \"strategy_name\": \"normal\",\n",
    "        \"confidence_level\": 0.95,\n",
    "        \"use_fpc\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7dd280",
   "metadata": {},
   "source": [
    "### Create Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f26d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(config_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa186134",
   "metadata": {},
   "source": [
    "### Run Multi-Config Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
    "results = experiment.run_evals(\n",
    "    config_group=config_group,\n",
    "    dataset=scifact_dataset,\n",
    "    num_actors=2,\n",
    "    num_shards=4,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ce33b",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results dict to DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
    "    for run_id, (_, metrics_dict) in results.items()\n",
    "])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135d951",
   "metadata": {},
   "source": [
    "### End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09265e66",
   "metadata": {},
   "source": [
    "### View RapidFire AI Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05379a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment-specific log file\n",
    "log_file = experiment.get_log_file_path()\n",
    "\n",
    "print(f\"üìÑ Log File: {log_file}\")\n",
    "print()\n",
    "\n",
    "if log_file.exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Last 30 lines of {log_file.name}:\")\n",
    "    print(\"=\" * 80)\n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[-30:]:\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print(f\"‚ùå Log file not found: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
