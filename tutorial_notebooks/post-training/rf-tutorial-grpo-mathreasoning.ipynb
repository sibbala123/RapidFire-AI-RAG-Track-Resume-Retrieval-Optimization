{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Join Discord if you need help + \u2b50 <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> \u2b50\n",
    "<br/>\n",
    "To install RapidFire AI on your own machine, see the <a href=\"https://oss-docs.rapidfire.ai/en/latest/walkthrough.html\">Install and Get Started</a> guide in our docs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RapidFire AI Tutorial Use Case: GRPO for Math Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfireai import Experiment\n",
    "from rapidfireai.fit.automl import List, RFGridSearch, RFModelConfig, RFLoraConfig, RFGRPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Specify Train and Eval Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] \n",
    "    return data \n",
    "\n",
    "# Select a subset of the dataset for demo purposes\n",
    "train_dataset = get_gsm8k_questions(split=\"train\").select(range(500))\n",
    "eval_dataset = get_gsm8k_questions(split=\"test\").select(range(100))\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "eval_dataset =  eval_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_formatting_function(row):\n",
    "    \"\"\"Function to preprocess each example from dataset\"\"\"\n",
    "\n",
    "    def extract_hash_answer(text: str) -> str | None:\n",
    "        if \"####\" not in text:\n",
    "            return None\n",
    "        answer = text.split(\"####\")[1].strip()\n",
    "        try:\n",
    "            answer = answer.replace(\",\", \"\")\n",
    "        except:\n",
    "            return None\n",
    "        return answer\n",
    "        \n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "    Respond in the following format:\n",
    "    <reasoning>\n",
    "    ...\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    ...\n",
    "    </answer>\n",
    "    \"\"\"\n",
    "    return { # Return a conversation format dictionary\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': row['question']}\n",
    "        ],\n",
    "        'question': row['question'],\n",
    "        'answer': extract_hash_answer(row['answer'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every experiment instance must be uniquely named\n",
    "experiment = Experiment(experiment_name=\"exp1-math-reasoning\", mode=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "\n",
    "    def extract_xml_answer(text: str) -> str:\n",
    "        answer = text.split(\"<answer>\")[-1]\n",
    "        answer = answer.split(\"</answer>\")[0]\n",
    "        return answer.strip()\n",
    "\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # x('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \n",
    "    def extract_xml_answer(text: str) -> str:\n",
    "        answer = text.split(\"<answer>\")[-1]\n",
    "        answer = answer.split(\"</answer>\")[0]\n",
    "        return answer.strip()\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    import re\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    import re\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    def count_xml(text) -> float:\n",
    "        count = 0.0\n",
    "        if text.count(\"<reasoning>\\n\") == 1:\n",
    "            count += 0.125\n",
    "        if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "            count += 0.125\n",
    "        if text.count(\"\\n<answer>\\n\") == 1:\n",
    "            count += 0.125\n",
    "            count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "        if text.count(\"\\n</answer>\") == 1:\n",
    "            count += 0.125\n",
    "            count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "        return count\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Multi-Config Knobs for Model, LoRA, and GRPO Trainer using RapidFire AI Wrapper APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = RFLoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "grpo_config1 = RFGRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.1,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4, \n",
    "    num_generations=8,\n",
    "    optim =\"adamw_8bit\",\n",
    "    num_train_epochs=2,\n",
    "    max_prompt_length=1024,\n",
    "    max_completion_length=1024,\n",
    "    logging_steps=2,\n",
    "    eval_steps=5\n",
    ")\n",
    "\n",
    "grpo_config2 = grpo_config1.copy()\n",
    "grpo_config2.learning_rate = 1e-5\n",
    "\n",
    "reward_funcs = [\n",
    "    correctness_reward_func,\n",
    "    int_reward_func,\n",
    "    strict_format_reward_func,\n",
    "    soft_format_reward_func,\n",
    "    xmlcount_reward_func,\n",
    "]\n",
    "\n",
    "# List of 4 separate configs\n",
    "config_set = List([\n",
    "    RFModelConfig(\n",
    "        model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config1,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"right\", \"truncation\": True}\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config1,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"right\", \"truncation\": True}\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config2,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"right\", \"truncation\": True}\n",
    "    ),\n",
    "    RFModelConfig(\n",
    "        model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        peft_config=lora_config,\n",
    "        training_args=grpo_config1,\n",
    "        formatting_func=sample_formatting_function,\n",
    "        reward_funcs=reward_funcs,\n",
    "        model_kwargs={\"load_in_4bit\": True, \"device_map\": \"auto\", \"torch_dtype\": \"auto\", \"use_cache\": False},\n",
    "        tokenizer_kwargs={\"model_max_length\": 2048, \"padding_side\": \"right\", \"truncation\": True}\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_create_model(model_config):\n",
    "   \"\"\"Function to create model object for any given config; must return tuple of (model, tokenizer)\"\"\"\n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "   \n",
    "   model_name = model_config[\"model_name\"]\n",
    "   model_kwargs = model_config[\"model_kwargs\"]\n",
    "   tokenizer_kwargs = model_config[\"tokenizer_kwargs\"]\n",
    "   return (\n",
    "      AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs),\n",
    "      AutoTokenizer.from_pretrained(model_name, **tokenizer_kwargs)\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Config Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search across all sets of config knob values = 4 combinations in total\n",
    "config_group = RFGridSearch(\n",
    "    configs=config_set,\n",
    "    trainer_type=\"GRPO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi-Config Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training of all configs in the config_group with swap granularity of 4 chunks\n",
    "experiment.run_fit(config_group, sample_create_model, train_dataset, eval_dataset, num_chunks=4, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Current Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/RapidFire - Blue bug -white text.svg\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/6vSTtncKNN\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/discord-button.svg\" width=\"145\"></a>\n",
    "<a href=\"https://oss-docs.rapidfire.ai/\"><img src=\"https://raw.githubusercontent.com/RapidFireAI/rapidfireai/main/doc/images/documentation-button.svg\" width=\"125\"></a>\n",
    "<br/>\n",
    "Thanks for trying RapidFire AI! \u2b50 <i>Star us on <a href=\"https://github.com/RapidFireAI/rapidfireai\">GitHub</a></i> \u2b50\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}